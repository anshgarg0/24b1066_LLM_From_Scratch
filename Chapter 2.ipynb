{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3262ed",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e074f",
   "metadata": {},
   "source": [
    "## 2.1: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3e2c1",
   "metadata": {},
   "source": [
    "Neural Networks or any ML Models need numbers as their inputs so we convert words into a high dimensional vectors which represent them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e7158",
   "metadata": {},
   "source": [
    "## 2.2: Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef75449",
   "metadata": {},
   "source": [
    " Breaking up all of the text data into tokens which is basically the text segregated into words and punctuations. To demonstrate this here, we use a short story. It is stored in verdict.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a311417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"verdict.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096de321",
   "metadata": {},
   "source": [
    "Now, we split it on the basis of whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e8a258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that,', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory,', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting,', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow,', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on', ' ', 'the', ' ', 'Riviera.', ' ', '(Though', ' ', 'I']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1439bc5",
   "metadata": {},
   "source": [
    "To enhance this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "632cdfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius--though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough--so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a', ' ', 'villa', ' ', 'on']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ea564",
   "metadata": {},
   "source": [
    "To cover more special characters, and get rid of ' ' tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cff7ad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c58ad",
   "metadata": {},
   "source": [
    "## 2.3 Token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d421fb",
   "metadata": {},
   "source": [
    "Generating a vocabulary which contains unique tokens sorted lexicographically, mapped to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6dd04e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(list(set(result)))\n",
    "print(len(tokens))\n",
    "vocab = {token:integer for integer, token in enumerate(tokens)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6176d",
   "metadata": {},
   "source": [
    "Creating a Tokenizer class, which takes in a vocabulary as input and has encode and decode methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94925383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "test_data = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(test_data)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64100f20",
   "metadata": {},
   "source": [
    "## 2.4 Adding Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c8a02",
   "metadata": {},
   "source": [
    "It is beneficial to add some special tokens to help with contextual inference. Here we implement two such tokens <|unk|> and <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa543430",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "vocab = {token:id for id, token in enumerate(tokens)}\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] if s in self.str_to_int else self.str_to_int[\"<|unk|>\"] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "test_data = \" <|endoftext|> \".join((text1, text2))\n",
    "print(test_data)\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cdf91",
   "metadata": {},
   "source": [
    "## 2.5: Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e543c0",
   "metadata": {},
   "source": [
    "A sophisticated method for tokenising used in GPT-2, GPT-3. Uses the tiktoken library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "test_data_ids = tokenizer.encode(test_data, allowed_special={\"<|endoftext|>\"})\n",
    "print(test_data_ids)\n",
    "print(tokenizer.decode(test_data_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258ac17",
   "metadata": {},
   "source": [
    "Handling unknown words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39:  H\n",
      "461:  ak\n",
      "28241:  dj\n",
      "69:  f\n",
      "3099:  ha\n",
      "264:   s\n",
      "28241:  dj\n",
      "69:  f\n",
      "74:  k\n",
      "1443:  bs\n",
      "67:  d\n"
     ]
    }
   ],
   "source": [
    "x = tokenizer.encode(\"Hakdjfha sdjfkbsd\")\n",
    "for i in x:\n",
    "    print(f\"{i}: \", tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2081957c",
   "metadata": {},
   "source": [
    "## 2.6: Data Sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bcc5f",
   "metadata": {},
   "source": [
    "Here we generate the input and target pairs required for training our model to predict next word. We use a sliding window approach.\n",
    "\n",
    "max_length = the number of tokens in input and output\n",
    "\n",
    "stride = the difference between token indices of adjacent samples\n",
    "\n",
    "at the ith iteration, our input sample is from i to i+max_length\n",
    "our output sample is from i+1 to i+max_length+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627e837",
   "metadata": {},
   "source": [
    "Preparing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ee59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"verdict.txt\", 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b0926",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45226d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62abafe",
   "metadata": {},
   "source": [
    "Creating our Dataset Class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_samples = []\n",
    "        self.output_samples = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            self.input_samples.append(torch.tensor(token_ids[i:i+max_length]))\n",
    "            self.output_samples.append(torch.tensor(token_ids[i+1:i+max_length+1]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_samples[idx], self.output_samples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837712c8",
   "metadata": {},
   "source": [
    "Method for creating a dataloader from it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True,num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6c489",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0c03b",
   "metadata": {},
   "source": [
    "# 2.7: Initialising Token imbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b973a1",
   "metadata": {},
   "source": [
    "Illustrative Example:\n",
    "\n",
    "Suppose our vocab has 6 tokens and we want the embedding to be three dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fad0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84e50a",
   "metadata": {},
   "source": [
    "This is 6x3 matrix where each row represents the embedding for the respective token. It is randomised for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43659178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9d4ca",
   "metadata": {},
   "source": [
    "Essentially this is a look up matrix. As seen here the value corresponding to the 3rd index in the weights is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ec1a9",
   "metadata": {},
   "source": [
    "## 2.8: Encoding Word Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b62a1f",
   "metadata": {},
   "source": [
    "Tranformers are position-agnostic because they process all tokens parallely, thus we require a way to encode positional information into the input embeddings by operating on the token embeddings. There are two approaches to this: Relative or Absolute. In the absolute approach, each position in a sequence has a embedding of the same dimension, which is added on to the token embedding of the token at the position. In the relative approach, positional embeddings are determined on the basis of a token's distance from other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ac05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# More Practical Parameters and Vocab Size taken from our tokeniser\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Loading Dataset\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "# Printing token embedding from first batch\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa6946",
   "metadata": {},
   "source": [
    "Now we shall create the positional embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044fe7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) # each sample has 4 tokens meaning each sample has 4 positions exactly\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99a084",
   "metadata": {},
   "source": [
    "Finally, we will add the positional embeddings to the token embeddings to generate the input embeddings which will be fed to transformers in our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
