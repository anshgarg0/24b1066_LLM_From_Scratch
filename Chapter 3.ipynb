{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d4d78c",
   "metadata": {},
   "source": [
    "# Chapter - 3: Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fcef9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In this chapter, we focus on building Attention Mechanisms which are at the heart of modern LLMs. They equip our models with the ability to understand relationships between words and allow for more contextual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cab047",
   "metadata": {},
   "source": [
    "## 3.3: Attending to different parts of the input with self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ad288",
   "metadata": {},
   "source": [
    "Firstly we calculate the attention scores, which is just the dot product of the embeddings of query token with all other tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "885c8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i in range(6):\n",
    "    attention_scores_2[i] = torch.dot(inputs[1], inputs[i])\n",
    "\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8c786",
   "metadata": {},
   "source": [
    "Next, we calculate the attention weights which are just normalised attention scores so that their sum = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a8a301",
   "metadata": {},
   "source": [
    "We'll se two ways to do that:\n",
    "1. x[i]/x.sum\n",
    "2. e^x[i]/e^x.sum (SOFTMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a98a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656]) tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2_linear = attention_scores_2/attention_scores_2.sum()\n",
    "attention_weights_2_softmax = torch.softmax(attention_scores_2, dim = 0, dtype=float)\n",
    "print(attention_weights_2_linear, attention_weights_2_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33f392",
   "metadata": {},
   "source": [
    "Now, finally we take the weighted sum of all input tokens to generate the context vector for inputs[1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5534afd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = torch.zeros(inputs.shape[1])\n",
    "for i in range(inputs.shape[0]):\n",
    "    context_vector_2 += inputs[i]*attention_weights_2_softmax[i]\n",
    "\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e24e5",
   "metadata": {},
   "source": [
    "### Generating the context vector for entire input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9deae4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "attention_score = inputs @ inputs.T\n",
    "attentions_weights = torch.softmax(attention_score, dim=1)\n",
    "\n",
    "context_vector = attentions_weights @ inputs\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b4584",
   "metadata": {},
   "source": [
    "## 3.4: Attention Mechanism with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958cef00",
   "metadata": {},
   "source": [
    "This works by having three weight matrices, Wk, Wq and Wv. Query, Key and Value. For calculating context vector for inputs[2], its query vector is scalarly multiplied by the key vector of others to get the attention scores. Normalising it yields the attention weights, and the weighted sum of value vectors results in the context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4dad56a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # generally these are same\n",
    "\n",
    "torch.manual_seed(123)\n",
    "query_mat = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "key_mat = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "value_mat = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "query_2 = inputs[1] @ query_mat\n",
    "keys = inputs @ key_mat\n",
    "values = inputs @ value_mat\n",
    "attention_scores_2 = query_2 @ keys.T\n",
    "\n",
    "attentions_weights = torch.softmax(attention_scores_2/d_out**0.5, dim = 0) # scalable with dimension of embeddings\n",
    "context_vector_2 = attentions_weights @ values\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a19741",
   "metadata": {},
   "source": [
    "Now, to generate context vectors for all of the input tokens, we will create a attention class, using the nn.Module class as Parent which gives us easy functionality when we will be required to optimize weights using gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33caa823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195fd6d",
   "metadata": {},
   "source": [
    "Now we switch nn.Parameters for nn.Linear. It is beneficial as it has a better weight initialisation scheme, leading to more appropriate initial weights. This helps in training the model easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a00ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # the bias parameters sets bias to 0, so it behaves purely as a matrix multiplication\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61ac4b",
   "metadata": {},
   "source": [
    "## 3.5: Hiding duture words with casual attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf38d0",
   "metadata": {},
   "source": [
    "Since our model learns by next word prediction, it would literally be cheating if the context vector for the current word had data from the future tokens. Thus, we use masks to get rid of future attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3961798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_score = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_score/d_out**0.5, dim=1)\n",
    "\n",
    "context_length = attn_weights.shape[0]\n",
    "casual_mask = torch.tril(torch.ones(context_length, context_length))\n",
    "masked_weights = casual_mask*attn_weights\n",
    "print(masked_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4898d5",
   "metadata": {},
   "source": [
    "We have to normalise this again, When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation However, the key insight is that when we renormalize the attention weights after masking, what we're essentially doing is recalculating the softmax over a smaller subset (since masked positions don't contribute to the softmax value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c60a15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "normalised_masked_weights = masked_weights / masked_weights.sum(dim=1, keepdim=True)\n",
    "print(normalised_masked_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915f65f",
   "metadata": {},
   "source": [
    "It is better to fill the future attentionw weights with negative infinity, instead of zero. This allows us to use the softmax function for normalisation rather than linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00c196aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked_weights = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked_weights)\n",
    "\n",
    "normalised_masked_weights = torch.softmax(masked_weights/d_out**0.5, dim = 1)\n",
    "print(normalised_masked_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a089a",
   "metadata": {},
   "source": [
    "Dropping out weights to reduce overfitting issue, and multiplies remaining by the reciprocal of dropout rate to deal with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73c96d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.4638, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.3968, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3869, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "DropOut = torch.nn.Dropout(0.5)\n",
    "normalised_masked_weights = DropOut(normalised_masked_weights)\n",
    "print(normalised_masked_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5802a",
   "metadata": {},
   "source": [
    "Now we consolidate everything to a class that can even handle data generated by the DataLoader Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19624039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttentionV1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_keys = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_values = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.DropOut = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        attn_score = self.W_query(x) @ self.W_keys(x).transpose(1,2)\n",
    "        attn_score.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf )\n",
    "        attn_weights = torch.softmax(attn_score / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.DropOut(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ self.W_values(x)\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # stacked inputs to mimc epoch\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttentionV1(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca85121",
   "metadata": {},
   "source": [
    "## 3.6: Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041637d",
   "metadata": {},
   "source": [
    "Utilising several CausalAttention Classes.\n",
    "\n",
    "Our first implementation will be a wrapper for multiple heads, in which the final context vector will be a concatenation of the outputs of multiple heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cca5f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttentionV1(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f71be",
   "metadata": {},
   "source": [
    "Now, we try to implement this more efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a8d3f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # d_out per head\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out) // this d_out = num_heads*head_dim\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Unfurling the last dimensions into 2 to reveal the seperated heads\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        # This now segregates heads into tokens not otherwise\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim) Switching dim 1 and 2 so that we can reduce dimension size properly\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) #tensor.contiguous() has to do with memory allocation\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
